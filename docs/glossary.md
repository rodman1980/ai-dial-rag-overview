---
title: Glossary
description: Domain terms, technical concepts, and abbreviations used in the RAG system
version: 1.0.0
last_updated: 2025-12-30
related: [README.md, architecture.md]
tags: [glossary, terminology, definitions]
---

# Glossary

## RAG Concepts

### RAG (Retrieval-Augmented Generation)
A natural language processing technique that combines information retrieval with text generation. The system retrieves relevant documents from a knowledge base and uses them to augment a prompt before generating an answer with a Large Language Model (LLM).

**Usage in Project**: The three-stage pipeline (Retrieval → Augmentation → Generation) that powers the microwave assistant.

**References**: [Architecture: RAG Pipeline](./architecture.md#system-overview)

---

### Vector Embedding
A numerical representation of text as a high-dimensional vector. Semantically similar texts have similar vector representations, enabling similarity-based search.

**Example**: The query "How do I clean?" and the manual text "To clean the microwave..." would have similar embeddings.

**In Project**: Generated by `text-embedding-3-small-1` model (1536 dimensions).

**References**: [Architecture: Vector Index](./architecture.md#4-vector-index-microwave_faiss_index)

---

### Similarity Search
A technique to find items most similar to a query based on vector distance metrics (e.g., cosine similarity, L2 distance).

**In Project**: FAISS performs L2 distance-based similarity search to find relevant manual chunks.

**References**: [API: retrieve_context](./api.md#retrieve_contextquery-k4-score03)

---

### Chunk
A segment of a larger document, created by splitting the original text into smaller, semantically coherent pieces.

**In Project**: Microwave manual split into ~300-character chunks with 50-character overlap.

**Purpose**: Balance between providing enough context and maintaining specificity for retrieval.

**References**: [Architecture: Document Chunking](./architecture.md#why-recursivecharactertextsplitter)

---

## LangChain Components

### LangChain
An open-source framework for building applications with Large Language Models (LLMs). Provides abstractions for document loading, text splitting, embeddings, vector stores, and LLM integrations.

**Used Modules**:
- `langchain_community`: Community-contributed loaders and vector stores
- `langchain_openai`: Azure OpenAI integrations
- `langchain_text_splitters`: Document chunking utilities
- `langchain_core`: Core abstractions (messages, vector stores)

---

### TextLoader
A LangChain utility to load text files into document objects.

**In Project**: Loads `microwave_manual.txt` with UTF-8 encoding.

**Code**: `task/app.py`, line ~62

---

### RecursiveCharacterTextSplitter
A LangChain text splitter that recursively splits text using a hierarchy of separators (e.g., `\n\n` → `\n` → `.`).

**In Project**: Configured with `chunk_size=300`, `chunk_overlap=50`.

**Purpose**: Preserve semantic coherence while creating manageable chunks.

**References**: [ADR-003: Chunking Strategy](./adr/ADR-003-chunking-strategy.md)

---

### VectorStore
An abstract LangChain interface for vector databases. Provides methods like `similarity_search()`, `add_documents()`, `save_local()`.

**Implementation**: FAISS (Facebook AI Similarity Search)

**In Project**: Stores and retrieves microwave manual embeddings.

---

### AzureOpenAIEmbeddings
LangChain client for Azure OpenAI embedding models. Converts text to vectors using the specified deployment.

**Configuration**:
- Deployment: `text-embedding-3-small-1`
- Endpoint: DIAL API (`https://ai-proxy.lab.epam.com`)
- Dimensions: 1536

---

### AzureChatOpenAI
LangChain client for Azure OpenAI chat completion models. Generates text based on message history.

**Configuration**:
- Deployment: `gpt-4o`
- Temperature: `0.0` (deterministic)
- Endpoint: DIAL API

---

## FAISS Concepts

### FAISS (Facebook AI Similarity Search)
A library for efficient similarity search and clustering of dense vectors. Supports exact and approximate nearest neighbor search.

**In Project**: Used for in-memory similarity search with exact L2 distance.

**Index Type**: Flat index (brute-force search, exact results)

**References**: [ADR-001: FAISS for Vector Storage](./adr/ADR-001-faiss-vector-storage.md)

---

### Index (FAISS)
A data structure optimized for fast similarity search. Stores vectors and metadata.

**In Project**: Persisted to `microwave_faiss_index/index.faiss` and `index.pkl`.

**Creation Time**: ~30 seconds (one-time)

**Loading Time**: ~2 seconds (subsequent runs)

---

### L2 Distance (Euclidean Distance)
A distance metric measuring straight-line distance between two vectors in Euclidean space. Lower distance = higher similarity.

**Formula**: `d = sqrt(sum((x_i - y_i)^2))`

**In Project**: FAISS default distance metric for similarity search.

---

### Relevance Score
A normalized similarity score (0.0 to 1.0) indicating how relevant a chunk is to the query. Higher score = more relevant.

**In Project**: Threshold of `0.3` filters out low-relevance chunks.

**Calculation**: Depends on FAISS distance metric (inverse of L2 distance, normalized).

---

## Azure OpenAI / DIAL

### DIAL (AI Proxy)
EPAM's internal proxy for accessing Azure OpenAI services. Provides unified API for embeddings and chat completions.

**Endpoint**: `https://ai-proxy.lab.epam.com`

**Access**: Requires EPAM VPN and API key

**References**: [Setup: API Key](./setup.md#api-key)

---

### Azure OpenAI
Microsoft's enterprise version of OpenAI's models, hosted on Azure cloud infrastructure.

**Used Models**:
- `text-embedding-3-small-1`: Embedding model (1536 dimensions)
- `gpt-4o`: Chat completion model (GPT-4 optimized)

---

### Deployment
An Azure OpenAI resource identifier for a specific model instance. Maps to model name and version.

**Example**: `gpt-4o` deployment refers to a specific GPT-4 optimized model.

---

### Temperature
A parameter (0.0 to 2.0) controlling randomness in LLM generation. Lower = more deterministic, higher = more creative.

**In Project**: Set to `0.0` for consistent, factual answers.

---

### Token
The unit of text processing for LLMs. Roughly 4 characters or 0.75 words in English.

**Example**: "Hello world" = 2 tokens

**In Project**: Relevant for API costs and context limits.

---

## System Prompts

### System Prompt
Instructions to the LLM defining its role, behavior, and constraints. Prepended to all conversations.

**In Project**: Instructs LLM to answer only from RAG context and cite sources.

**Code**: `SYSTEM_PROMPT` in `task/app.py`, line ~12

---

### User Prompt
The actual query or message from the user, formatted with context.

**In Project**: Template includes `RAG CONTEXT` and `USER QUESTION` sections.

**Code**: `USER_PROMPT` in `task/app.py`, line ~24

---

### Augmented Prompt
A user prompt enhanced with retrieved context. Combines query + relevant chunks.

**Purpose**: Ground LLM's answer in specific, retrieved information.

**References**: [API: augment_prompt](./api.md#augment_promptquery-context)

---

## Domain-Specific Terms

### DW 395 HCG
The microwave oven model covered by the knowledge base manual.

**Specifications**: TODO (add after extracting from manual)

---

### ECO Function
An energy-saving mode on the DW 395 HCG microwave.

**Purpose**: Reduce power consumption during standby.

**Activation**: TODO (query the RAG system for exact steps)

---

### Multi-Stage Cooking
A feature allowing multiple cooking programs in sequence (e.g., defrost → cook → grill).

**Limitations**: Certain modes cannot be combined (specified in manual).

---

### FAISS Index Deserialization
The process of loading a saved FAISS index from disk into memory.

**Security Note**: `allow_dangerous_deserialization=True` required for pickle-based index loading. Safe for trusted local indexes only.

**Code**: `task/app.py`, line ~47

---

## Performance Metrics

### Retrieval Latency
Time taken to find and return relevant chunks from the vector store.

**Target**: <500ms (includes embedding + search)

**Typical**: ~210ms (200ms embedding, 10ms search)

---

### Generation Latency
Time taken for the LLM to generate an answer.

**Target**: <5s

**Typical**: 2-5s (depends on response length and DIAL API load)

---

### Chunk Overlap
The number of characters that appear in both adjacent chunks during text splitting.

**In Project**: 50 characters

**Purpose**: Prevent context loss at chunk boundaries.

---

## Error Codes

### FileNotFoundError
Python exception raised when `microwave_manual.txt` is missing.

**Solution**: Ensure file exists at `task/microwave_manual.txt`.

**References**: [Setup: Troubleshooting](./setup.md#2-filenotfounderror-the-file-microwave_manualtxt-does-not-exist)

---

### 401 Unauthorized
HTTP error indicating invalid or missing DIAL API key.

**Solution**: Set `DIAL_API_KEY` environment variable.

**References**: [Setup: API Key](./setup.md#3-401-unauthorized-from-dial-api)

---

### 429 Rate Limit
HTTP error indicating too many API requests in a short period.

**Current Handling**: None (TODO: add retry logic)

**References**: [Roadmap: Error Handling](./roadmap.md)

---

## Abbreviations

| Abbreviation | Full Term |
|--------------|-----------|
| **RAG** | Retrieval-Augmented Generation |
| **LLM** | Large Language Model |
| **FAISS** | Facebook AI Similarity Search |
| **DIAL** | (EPAM AI Proxy, exact acronym meaning unclear) |
| **API** | Application Programming Interface |
| **JSON** | JavaScript Object Notation |
| **UTF-8** | Unicode Transformation Format - 8-bit |
| **VPN** | Virtual Private Network |
| **CLI** | Command-Line Interface |
| **ADR** | Architecture Decision Record |

---

## Related Concepts (External)

### Cosine Similarity
An alternative distance metric measuring the cosine of the angle between two vectors. Range: [-1, 1], where 1 = identical.

**In Project**: Not used (FAISS uses L2 distance by default).

---

### Prompt Engineering
The practice of crafting effective prompts to elicit desired responses from LLMs.

**In Project**: System and user prompt templates designed to enforce grounding.

---

### Hallucination (LLM)
When an LLM generates plausible-sounding but factually incorrect information.

**Mitigation in Project**: RAG architecture grounds answers in retrieved context; system prompt instructs to decline if context insufficient.

---

### Zero-Shot Learning
LLM's ability to perform tasks without specific training examples.

**In Project**: LLM generates answers without fine-tuning on microwave domain (relies on RAG context).

---

**Related Documentation**:
- [Architecture](./architecture.md) - Technical concepts in context
- [API Reference](./api.md) - Method-specific terminology
- [Testing Guide](./testing.md) - Quality metrics and validation
